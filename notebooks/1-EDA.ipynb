{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22ba5640",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34cf012f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/data/horse/ws/s4610340-gentle-grasp/gentle-grasping')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_ws = Path.cwd().parent\n",
    "path_ws"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f707b0",
   "metadata": {},
   "source": [
    "Unsupervised labels example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac11011f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load(path_ws / \"data/raw/data_gentle_grasping/collected_data/data_2024_07_13_114803/labels.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7823d08",
   "metadata": {},
   "source": [
    "Supervised labels example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30337c33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load(path_ws / \"data/raw/data_gentle_grasping/collected_data/data_2024_07_13_114803/labels_supervised.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccc4f07",
   "metadata": {},
   "source": [
    "Dataset shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed29ef7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torch.load(path_ws / \"data/raw/data_gentle_grasping/gentle_grasping_dataset.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b90ddad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vision images shape:\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "Touch images shape:\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "Actions shape:\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1, 4])\n",
      "Labels shape: torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "i, (vision_imgs, touch_imgs, actions, labels)  = next(enumerate(data_loader))\n",
    "\n",
    "\n",
    "print(\"Vision images shape:\")\n",
    "for img in vision_imgs:\n",
    "    print(img.shape)\n",
    "print(\"Touch images shape:\")\n",
    "for img in touch_imgs:\n",
    "    print(img.shape)\n",
    "print(\"Actions shape:\")\n",
    "for a in actions:\n",
    "    print(a.shape)\n",
    "print(\"Labels shape:\", labels.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
